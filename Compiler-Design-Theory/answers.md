# Compiler Design - Answer Key (Questions 257-264)

# Q186 — Grammar ambiguity demonstration and parse tree for 'aa+a*'

## Key Points
- A grammar is ambiguous if a string has multiple parse trees
- The given grammar S → SS+ | SS* | a is ambiguous (postfix notation)
- 'aa+a*' can be derived and parsed

## Explanation

A grammar is **ambiguous** if there exists at least one string that can be generated by the grammar with two or more distinct parse trees (or equivalently, two or more leftmost/rightmost derivations).

The grammar S → SS+ | SS* | a uses postfix notation. To generate 'aa+a*':

**Derivation:**
```
S ⇒ SS* 
  ⇒ SS+S*
  ⇒ aS+S*
  ⇒ aa+S*
  ⇒ aa+a*
```

## Parse Tree

```
        S
       /|\
      S S *
     /|\ |
    S S + a
    | |
    a a
```

The string is built bottom-up: first 'a', then 'a', then '+' combines them, then another 'a', then '*' combines the result with 'a'.


# Q187 — Grammar ambiguity and parse tree for '((a))'

## Key Points
- Grammar S → (S) | S+S | a is ambiguous due to S+S production
- '((a))' can be generated using nested parentheses
- Parse tree shows two levels of parenthesization

## Explanation

The grammar S → (S) | S+S | a is ambiguous because strings involving the '+' operator can have multiple parse trees depending on grouping.

To generate '((a))':

**Derivation:**
```
S ⇒ (S)
  ⇒ ((S))
  ⇒ ((a))
```

## Parse Tree

```
      S
     /|\
    ( S )
     /|\
    ( S )
      |
      a
```

The derivation uses the production S → (S) twice, wrapping the terminal 'a' in two layers of parentheses. This demonstrates nested structure handling in the grammar.


# Q188 — LR(0) automaton for pointer/assignment grammar

## Key Points
- Build canonical LR(0) collection of items
- Compute GOTO function for state transitions
- Grammar: S → L=R | L, L → *R | id, R → L

## Explanation

To construct the LR(0) automaton, we augment the grammar with S' → S and build the collection of LR(0) items.

**Augmented Grammar:**
```
S' → S
S → L=R
S → L
L → *R
L → id
R → L
```

## LR(0) Automaton

```
I0: S' → •S
    S → •L=R
    S → •L
    L → •*R
    L → •id

I1: S' → S•

I2: S → L•=R
    S → L•

I3: L → *•R
    R → •L
    L → •*R
    L → •id

I4: L → id•

I5: S → L=•R
    R → •L
    L → •*R
    L → •id

I6: L → *R•

I7: R → L•

I8: S → L=R•
```

**Transitions:**
- I0 --S--> I1, --L--> I2, --*--> I3, --id--> I4
- I2 --=--> I5
- I3 --R--> I6, --L--> I7, --*--> I3, --id--> I4
- I5 --R--> I8, --L--> I7, --*--> I3, --id--> I4


# Q190 — LR(1) automaton for S → CC grammar

## Key Points
- LR(1) items include lookahead symbols
- Grammar: S' → S, S → CC, C → cC | d
- Each state contains items with lookaheads

## Explanation

LR(1) items have the form [A → α•β, a] where 'a' is the lookahead. We compute FIRST sets and propagate lookaheads.

**Augmented Grammar:**
```
S' → S
S → CC
C → cC | d
```

## LR(1) Automaton

```
I0: [S' → •S, $]
    [S → •CC, $]
    [C → •cC, c/d]
    [C → •d, c/d]

I1: [S' → S•, $]

I2: [S → C•C, $]
    [C → •cC, $]
    [C → •d, $]

I3: [C → c•C, c/d]
    [C → •cC, c/d]
    [C → •d, c/d]

I4: [C → d•, c/d]

I5: [S → CC•, $]

I6: [C → c•C, $]
    [C → •cC, $]
    [C → •d, $]

I7: [C → d•, $]

I8: [C → cC•, c/d]

I9: [C → cC•, $]
```

**Transitions:**
- I0: S→I1, C→I2, c→I3, d→I4
- I2: C→I5, c→I6, d→I7
- I3: C→I8, c→I3, d→I4
- I6: C→I9, c→I6, d→I7


# Q192 — Ambiguity demonstration: two leftmost derivations for 'id+id*id'

## Key Points
- Grammar E → E+E | E*E | -E | (E) | id is ambiguous
- 'id+id*id' has multiple parse trees (precedence undefined)
- Shows need for unambiguous expression grammars

## Explanation

The grammar is ambiguous because operators lack precedence and associativity rules.

**Leftmost Derivation 1 (+ first):**
```
E ⇒ E+E
  ⇒ id+E
  ⇒ id+E*E
  ⇒ id+id*E
  ⇒ id+id*id
```

**Leftmost Derivation 2 (* first):**
```
E ⇒ E*E
  ⇒ E+E*E
  ⇒ id+E*E
  ⇒ id+id*E
  ⇒ id+id*id
```

## Parse Trees

```
Tree 1 (+ binds looser):      Tree 2 (* binds looser):
        E                              E
       /|\                            /|\
      E + E                          E * E
      |  /|\                        /|\  |
     id E * E                      E + E id
        |   |                      |   |
       id  id                     id  id
```

Both trees are valid, demonstrating ambiguity. A proper expression grammar should enforce precedence (* before +).


# Q193 — Leftmost, rightmost derivations and ambiguity check for 'aa+a*'

## Key Points
- Grammar: S → SS+ | SS* | a (postfix notation)
- Both leftmost and rightmost derivations exist
- Grammar is ambiguous (multiple parse trees possible)

## Explanation

**Leftmost Derivation:**
```
S ⇒ SS*          (apply S → SS*)
  ⇒ SS+S*        (leftmost S → SS+)
  ⇒ aS+S*        (leftmost S → a)
  ⇒ aa+S*        (S → a)
  ⇒ aa+a*        (S → a)
```

**Rightmost Derivation:**
```
S ⇒ SS*          (apply S → SS*)
  ⇒ Sa*          (rightmost S → a)
  ⇒ SS+a*        (rightmost S → SS+)
  ⇒ Sa+a*        (rightmost S → a)
  ⇒ aa+a*        (S → a)
```

## Ambiguity Analysis

The grammar is **ambiguous**. For example, the string 'aaa*+' can be parsed as either ((aa)(a*))+ or (a(aa*))+, yielding different parse trees. The postfix notation allows multiple groupings without explicit precedence rules.


# Q194 — Regular expressions for various string languages

## Key Points
- Use concatenation, union, and Kleene star
- Consider divisibility, prefix/suffix conditions, and specific patterns
- Alphabet Σ = {a, b}

## Regular Expressions

**1. Strings whose length is divisible by 3:**
```
((a|b)(a|b)(a|b))*
```

**2. Strings that begin and end with different symbols:**
```
a(a|b)*b | b(a|b)*a | a | b
```

**3. Strings that end with 'aab':**
```
(a|b)*aab
```

**4. Strings that begin with 'bab':**
```
bab(a|b)*
```

**5. All even-length strings:**
```
((a|b)(a|b))*
```

## Explanation

- For divisibility by 3, group three symbols together and repeat
- Beginning/ending with different symbols: either a...b or b...a paths
- Ending/beginning patterns use (a|b)* for flexibility
- Even length groups symbols in pairs


# Q195 — Leftmost, rightmost derivations and parse tree for '(a)+(a)'

## Key Points
- Grammar: S → (S) | S+S | a
- Demonstrate both derivation types
- Construct unique parse tree

## Explanation

**Leftmost Derivation:**
```
S ⇒ S+S          (S → S+S)
  ⇒ (S)+S        (leftmost S → (S))
  ⇒ (a)+S        (S → a)
  ⇒ (a)+(S)      (S → (S))
  ⇒ (a)+(a)      (S → a)
```

**Rightmost Derivation:**
```
S ⇒ S+S          (S → S+S)
  ⇒ S+(S)        (rightmost S → (S))
  ⇒ S+(a)        (S → a)
  ⇒ (S)+(a)      (S → (S))
  ⇒ (a)+(a)      (S → a)
```

## Parse Tree

```
        S
       /|\
      S + S
     /|\  /|\
    ( S ) ( S )
      |     |
      a     a
```

The tree shows the addition of two parenthesized expressions. Both derivations produce the same parse tree, starting with S+S at the root.


# Q196 — Regular expressions for begin/end symbol conditions

## Key Points
- Alphabet Σ = {a, b}
- Different symbols: start ≠ end
- Same symbols: start = end

## Regular Expressions

**1. Strings that begin and end with different symbols:**
```
a(a|b)*b | b(a|b)*a | a | b
```

**Explanation:** Either start with 'a' and end with 'b', or start with 'b' and end with 'a'. Single symbols 'a' or 'b' also satisfy "different" vacuously in some interpretations, or exclude them if length > 1 is required: `a(a|b)*b | b(a|b)*a`.

**2. Strings that begin and end with the same symbol:**
```
a(a|b)*a | b(a|b)*b | a | b
```

**Explanation:** Either both ends are 'a' or both are 'b'. Single characters trivially begin and end with themselves.

## Notes

- For strings of length ≥ 2, remove single symbols from expressions
- Empty string ε begins and ends with "nothing" (same symbol) if included in language


# Q197 — Remove left recursion from grammar with ε-productions

## Key Points
- Grammar: A → ε | xAy | AA; S → Sa | aSSb | ε
- Apply standard left recursion elimination
- Handle ε-productions carefully

## Explanation

**Original Grammar:**
```
A → ε | xAy | AA
S → Sa | aSSb | ε
```

For **S → Sa | aSSb | ε:**
- Left-recursive production: S → Sa
- Non-recursive: aSSb | ε

**Eliminate left recursion from S:**
```
S → aSSbS' | εS' | S'
S' → aS' | ε
```

Simplifies to:
```
S → aSSbS' | S'
S' → aS' | ε
```

For **A → ε | xAy | AA:**
- Left-recursive production: A → AA
- Non-recursive: ε | xAy

**Eliminate left recursion from A:**
```
A → εA' | xAyA' | A'
A' → AA' | ε
```

Simplifies to:
```
A → xAyA' | A'
A' → AA' | ε
```

## Final Grammar

```
S → aSSbS' | S'
S' → aS' | ε
A → xAyA' | A'
A' → AA' | ε
```


# Q198 — Remove left recursion from multi-production grammar

## Key Points
- Grammar: A → Ab | aC; B → BaBB | BA; C → bC | BA
- Eliminate immediate left recursion per non-terminal
- Apply standard algorithm

## Explanation

**Original Grammar:**
```
A → Ab | aC
B → BaBB | BA
C → bC | BA
```

**For A → Ab | aC:**
- Left-recursive: Ab
- Non-recursive: aC

```
A → aCA'
A' → bA' | ε
```

**For B → BaBB | BA:**
- Left-recursive: BaBB, BA
- No non-recursive productions (problematic!)

Add ε-production or handle as:
```
B → (assume no valid strings without recursion)
```

If we must have a base case, this grammar is invalid as-is. Assuming intent for B:
```
B → AB'
B' → aBBB' | AB' | ε
```

**For C → bC | BA:**
- No left recursion (already right-recursive or non-recursive)

```
C → bC | BA
```

## Final Grammar

```
A → aCA'
A' → bA' | ε
B → AB'  (if A is defined)
B' → aBBB' | AB' | ε
C → bC | BA
```

**Note:** B's original grammar has no non-recursive production; this makes the language empty or requires external context.


# Q199 — Remove left recursion with interdependent non-terminals

## Key Points
- Grammar: S → Aa | Bb; A → Aa | Abc | c | Sb; B → bb
- Handle indirect left recursion (S → A → S)
- Order non-terminals: S, A, B

## Explanation

**Original Grammar:**
```
S → Aa | Bb
A → Aa | Abc | c | Sb
B → bb
```

**Step 1:** Order non-terminals as S, A, B. Eliminate S from A.

Substitute S → Aa | Bb into A → Sb:
```
A → Aa | Abc | c | Aab | Bbb
```

**Step 2:** Eliminate left recursion from A:
- Left-recursive: Aa, Aab, Abc
- Non-recursive: c, Bbb

```
A → cA' | BbbA'
A' → aA' | abcA' | bcA' | ε
```

Since B → bb:
```
A → cA' | bbbA'
A' → aA' | abcA' | bcA' | ε
```

**Step 3:** S remains:
```
S → Aa | Bb
```

## Final Grammar

```
S → Aa | Bb
A → cA' | bbbA'
A' → aA' | abcA' | bcA' | ε
B → bb
```


# Q200 — Remove left recursion from S → AA | 0; A → SS | 1

## Key Points
- Grammar: S → AA | 0; A → SS | 1
- Indirect left recursion between S and A
- Order: S before A, substitute and eliminate

## Explanation

**Original Grammar:**
```
S → AA | 0
A → SS | 1
```

**Step 1:** Order S, A. Substitute S into A → SS:
```
A → (AA|0)(AA|0) | 1
  = AAAA | AA0 | 0AA | 00 | 1
```

**Step 2:** Now substitute A back into S → AA:
```
S → (AAAA|AA0|0AA|00|1)(AAAA|AA0|0AA|00|1) | 0
```

This creates immediate left recursion in both. Apply elimination:

For **A** (no immediate left recursion yet), first compute S without A.

Alternative approach: rewrite with new non-terminals.

**Eliminate indirect recursion:**
```
S → 0S' | 1AS'
S' → AAAS' | A0S' | ε
A → 01 | 11
```

This requires careful expansion. A cleaner result:

## Final Grammar

```
S → 0S' | 1AS'
S' → AS' | ε
A → 0A' | 1A'
A' → SA' | ε
```

Substituting and simplifying fully yields a complex non-left-recursive form.


# Q201 — Remove left recursion from S → ABC grammar

## Key Points
- Grammar: S → ABC; A → Aa | Ad | b; B → Bd | e; C → Cc | g
- A, B, C each have immediate left recursion
- Eliminate per non-terminal independently

## Explanation

**Original Grammar:**
```
S → ABC
A → Aa | Ad | b
B → Bd | e
C → Cc | g
```

**For A → Aa | Ad | b:**
- Left-recursive: Aa, Ad
- Non-recursive: b

```
A → bA'
A' → aA' | dA' | ε
```

**For B → Bd | e:**
- Left-recursive: Bd
- Non-recursive: e

```
B → eB'
B' → dB' | ε
```

**For C → Cc | g:**
- Left-recursive: Cc
- Non-recursive: g

```
C → gC'
C' → cC' | ε
```

**S remains unchanged:**
```
S → ABC
```

## Final Grammar

```
S → ABC
A → bA'
A' → aA' | dA' | ε
B → eB'
B' → dB' | ε
C → gC'
C' → cC' | ε
```


# Q202 — Predictive parsing table for S → aABb grammar

## Key Points
- Grammar: S → aABb; A → c | ε; B → d | ε
- Compute FIRST and FOLLOW sets
- Check LL(1) condition (no conflicts)

## Explanation

**Grammar:**
```
S → aABb
A → c | ε
B → d | ε
```

**FIRST Sets:**
- FIRST(S) = {a}
- FIRST(A) = {c, ε}
- FIRST(B) = {d, ε}

**FOLLOW Sets:**
- FOLLOW(S) = {$}
- FOLLOW(A) = FIRST(B) ∪ {b} = {d, b}
- FOLLOW(B) = {b}

## Predictive Parsing Table

| Non-terminal | a | b | c | d | $ |
|--------------|---|---|---|---|---|
| S | S → aABb | | | | |
| A | | A → ε | A → c | A → ε | |
| B | | B → ε | | B → d | |

## LL(1) Analysis

No conflicts exist in the table. Each cell has at most one entry. **The grammar is LL(1)**.


# Q203 — Predictive parsing table and parse 'ab'

## Key Points
- Grammar: S → AaAb | BbBa; A → ε; B → ε
- Both A and B derive ε only
- Parse string 'ab' using table

## Explanation

**Grammar:**
```
S → AaAb | BbBa
A → ε
B → ε
```

**FIRST Sets:**
- FIRST(AaAb) = {a}
- FIRST(BbBa) = {b}

**FOLLOW Sets:**
- FOLLOW(S) = {$}
- FOLLOW(A) = {a, b}
- FOLLOW(B) = {b, a}

## Predictive Parsing Table

| Non-terminal | a | b | $ |
|--------------|---|---|---|
| S | S → AaAb | S → BbBa | |
| A | A → ε | A → ε | |
| B | B → ε | B → ε | |

## Parsing 'ab'

```
Stack        Input    Action
$S           ab$      S → AaAb
$bAaA        ab$      A → ε
$bAa         ab$      match a
$bA          b$       A → ε
$b           b$       match b
$            $        accept
```

The string 'ab' is **accepted**.


# Q204 — Prove LL(1) and parse 'aa'

## Key Points
- Grammar: S → SA | A; A → a
- Check for left recursion (present in S → SA)
- Must eliminate before proving LL(1)

## Explanation

**Original Grammar:**
```
S → SA | A
A → a
```

This grammar has **left recursion** (S → SA). It is **not LL(1)** as-is.

**Eliminate left recursion:**
```
S → AS'
S' → AS' | ε
A → a
```

**FIRST Sets:**
- FIRST(S) = {a}
- FIRST(S') = {a, ε}
- FIRST(A) = {a}

**FOLLOW Sets:**
- FOLLOW(S) = {$}
- FOLLOW(S') = {$}
- FOLLOW(A) = FIRST(S') ∪ {$} = {a, $}

## Predictive Parsing Table

| Non-terminal | a | $ |
|--------------|---|---|
| S | S → AS' | |
| S' | S' → AS' | S' → ε |
| A | A → a | |

## Parse 'aa'

```
Stack     Input   Action
$S        aa$     S → AS'
$S'A      aa$     A → a
$S'       a$      S' → AS'
$S'A      a$      A → a
$S'       $       S' → ε
$         $       accept
```

After left recursion elimination, the grammar is **LL(1)**.


# Q205 — Predictive parsing table for classic expression grammar

## Key Points
- Grammar: E → TE'; E' → +TE' | ε; T → FT'; T' → *FT' | ε; F → (E) | id
- This is the standard unambiguous expression grammar
- Demonstrates LL(1) properties with precedence

## Explanation

**FIRST Sets:**
- FIRST(E) = FIRST(T) = FIRST(F) = {(, id}
- FIRST(E') = {+, ε}
- FIRST(T') = {*, ε}

**FOLLOW Sets:**
- FOLLOW(E) = {), $}
- FOLLOW(E') = {), $}
- FOLLOW(T) = {+, ), $}
- FOLLOW(T') = {+, ), $}
- FOLLOW(F) = {*, +, ), $}

## Predictive Parsing Table

| Non-terminal | id | + | * | ( | ) | $ |
|--------------|-------|---|---|---|---|---|
| E | E → TE' | | | E → TE' | | |
| E' | | E' → +TE' | | | E' → ε | E' → ε |
| T | T → FT' | | | T → FT' | | |
| T' | | T' → ε | T' → *FT' | | T' → ε | T' → ε |
| F | F → id | | | F → (E) | | |

## LL(1) Status

No conflicts in the table. The grammar is **LL(1)**, properly encoding operator precedence and associativity.


# Q206 — Predictive parsing table for if-then-else grammar

## Key Points
- Grammar: S → iCtS | iCtSeS | a; C → b
- Classic dangling-else ambiguity
- Check for LL(1) conflicts

## Explanation

**Grammar:**
```
S → iCtS | iCtSeS | a
C → b
```

**FIRST Sets:**
- FIRST(S) = {i, a}
- FIRST(C) = {b}

**FOLLOW Sets:**
- FOLLOW(S) = {e, $}
- FOLLOW(C) = {t}

## Predictive Parsing Table

| Non-terminal | i | t | e | a | b | $ |
|--------------|---|---|---|---|---|---|
| S | S → iCtS<br>S → iCtSeS | | | S → a | | |
| C | | | | | C → b | |

## LL(1) Analysis

The entry for S under 'i' has **two productions**: S → iCtS and S → iCtSeS. This is a **conflict**.

The grammar is **not LL(1)** due to the dangling-else ambiguity. To make it LL(1), we must factor or disambiguate:

**Left-factored:**
```
S → iCtSS' | a
S' → eS | ε
C → b
```

After factoring, the grammar becomes LL(1).


# Q210 — Remove left recursion, construct parsing table, check LL(1)

## Key Points
- Grammar: A → Ac | Aad | bd | ε
- Eliminate left recursion from A
- Build predictive parsing table

## Explanation

**Original Grammar:**
```
A → Ac | Aad | bd | ε
```

**Eliminate left recursion:**
- Left-recursive: Ac, Aad
- Non-recursive: bd, ε

```
A → bdA' | A'
A' → cA' | adA' | ε
```

Since A → bdA' | A', and A' can be ε, we get:
```
A → bdA' | ε (effectively)
```

Simplified:
```
A → bdA' | εA'
```

Which gives:
```
A → bdA' | A'
A' → cA' | adA' | ε
```

**FIRST and FOLLOW:**
- FIRST(A) = {b, ε}
- FIRST(A') = {c, a, ε}
- FOLLOW(A) = {$}
- FOLLOW(A') = {$}

## Predictive Parsing Table

| Non-terminal | a | b | c | d | $ |
|--------------|---|---|---|---|---|
| A | A → A' | A → bdA' | A → A' | A → A' | A → A' |
| A' | A' → adA' | | A' → cA' | | A' → ε |

**Conflict:** A under 'a', 'c', 'd', and '$' has both productions A → bdA' and A → A' potentially applying.

The grammar is **not LL(1)** after left recursion elimination due to FIRST/FOLLOW conflicts with ε-productions.


# Q211 — Compute GOTO functions for expression grammar I₀

## Key Points
- Grammar: E' → E; E → E+T | T; T → T*F | F; F → (E) | id
- Compute GOTO(I₀, X) for X ∈ {E, T, id, (}
- Build LR(0) item sets

## Explanation

**I₀ (Initial State):**
```
E' → •E
E → •E+T
E → •T
T → •T*F
T → •F
F → •(E)
F → •id
```

## GOTO Computations

**GOTO(I₀, E):**
```
E' → E•
E → E•+T
```

**GOTO(I₀, T):**
```
E → T•
T → T•*F
```

**GOTO(I₀, id):**
```
F → id•
```

**GOTO(I₀, ( ):**
```
F → (•E)
E → •E+T
E → •T
T → •T*F
T → •F
F → •(E)
F → •id
```

Each GOTO moves the dot past the symbol and computes the closure of resulting items.


# Q213 — FIRST and FOLLOW for grammar A → Ab | aC; B → BaBB | BA; C → bC | BA

## Key Points
- Grammar has left recursion (must be noted)
- Compute FIRST considering terminals at production start
- FOLLOW uses production right-hand sides

## Explanation

**Grammar:**
```
A → Ab | aC
B → BaBB | BA
C → bC | BA
```

**FIRST Sets:**
- FIRST(A) = {a} (from A → aC; A → Ab derives from A which has 'a')
- FIRST(B) = {} (B is left-recursive with no base; undefined unless modified)
- FIRST(C) = {b} ∪ FIRST(B) ∪ FIRST(A) = {b, a} (from C → bC | BA)

If we assume corrections or ε-productions exist:
- FIRST(A) = {a}
- FIRST(B) = {a} (if B → BA and A starts with 'a')
- FIRST(C) = {b, a}

**FOLLOW Sets:**
- FOLLOW(A) = {b, a, $} (from A → Ab, B → BA, and C → BA)
- FOLLOW(B) = {a, $} (from B → BaBB and A → Ab)
- FOLLOW(C) = FOLLOW(A) = {b, a, $}

**Note:** This grammar is problematic due to left recursion and B having no non-recursive base production.


# Q214 — FIRST and FOLLOW for A → Ba | Baa; B → Ab | Abb

## Key Points
- Grammar: A → Ba | Baa; B → Ab | Abb
- Mutual recursion between A and B
- No base case (invalid grammar)

## Explanation

**Grammar:**
```
A → Ba | Baa
B → Ab | Abb
```

This grammar has **mutual left recursion** and no terminal-starting productions. It generates no strings (empty language).

**FIRST Sets:**
- FIRST(A) depends on FIRST(B)
- FIRST(B) depends on FIRST(A)
- No base case exists
- FIRST(A) = FIRST(B) = {} (empty)

**FOLLOW Sets:**
- FOLLOW(A) = {b, $} (from B → Ab | Abb)
- FOLLOW(B) = {a, $} (from A → Ba | Baa)

## Notes

This grammar is **non-productive**. It requires correction (adding base productions like A → a or B → b) to generate any strings. As written, FIRST sets are empty, making LL(1) analysis impossible.


# Q215 — FIRST and FOLLOW for S → Aa | Bb; A → Aa | Abc | c | Sb; B → bb

## Key Points
- Grammar: S → Aa | Bb; A → Aa | Abc | c | Sb; B → bb
- Has indirect left recursion (S → A → S)
- Compute sets carefully

## Explanation

**Grammar:**
```
S → Aa | Bb
A → Aa | Abc | c | Sb
B → bb
```

**FIRST Sets:**
- FIRST(S) = FIRST(A) ∪ FIRST(B) = {c, b}
- FIRST(A) = {c} ∪ FIRST(S) = {c, b} (from A → c and A → Sb)
- FIRST(B) = {b}

Wait, this creates circularity. More carefully:
- FIRST(B) = {b}
- FIRST(A) includes {c} (from A → c) and FIRST(S) (from A → Sb)
- FIRST(S) includes FIRST(A) and {b}

Resolving:
- FIRST(A) = {c}
- FIRST(S) = {c, b}
- Update: FIRST(A) = {c, c, b} = {c, b} (from A → Sb)

**FIRST:**
- FIRST(S) = {c, b}
- FIRST(A) = {c, b}
- FIRST(B) = {b}

**FOLLOW Sets:**
- FOLLOW(S) = {$} ∪ FOLLOW(A) (from A → Sb)
- FOLLOW(A) = {a, b, c, $} (from S → Aa, A → Aa, A → Abc)
- FOLLOW(B) = {b, $} (from S → Bb, B → bb)

**Final:**
- FOLLOW(S) = {$, a, b, c}
- FOLLOW(A) = {a, b, c, $}
- FOLLOW(B) = {$}


# Q216 — FIRST and FOLLOW for S → AA | 0; A → SS | 1

## Key Points
- Grammar: S → AA | 0; A → SS | 1
- Mutual recursion between S and A
- Both have base cases (0 and 1)

## Explanation

**Grammar:**
```
S → AA | 0
A → SS | 1
```

**FIRST Sets:**
- FIRST(S) includes FIRST(A) (from S → AA) and {0}
- FIRST(A) includes FIRST(S) (from A → SS) and {1}

Resolving:
- FIRST(A) = FIRST(S) ∪ {1}
- FIRST(S) = FIRST(A) ∪ {0}

Base: Start with terminals:
- FIRST(S) = {0, 1} (0 directly, 1 from A)
- FIRST(A) = {1, 0} (1 directly, 0 from S)

**FIRST:**
- FIRST(S) = {0, 1}
- FIRST(A) = {0, 1}

**FOLLOW Sets:**
- FOLLOW(S) = {$} ∪ FOLLOW(A) (from A → SS)
- FOLLOW(A) = FOLLOW(S) ∪ FOLLOW(A) (from S → AA and A → SS)

**Final:**
- FOLLOW(S) = {0, 1, $}
- FOLLOW(A) = {0, 1, $}

Both non-terminals have the same FIRST and FOLLOW sets due to symmetry.


# Q217 — FIRST and FOLLOW for S → ABC grammar

## Key Points
- Grammar: S → ABC; A → Aa | Ad | b; B → Bd | e; C → Cc | g
- Sequential production S → ABC
- Each of A, B, C has left recursion

## Explanation

**Grammar:**
```
S → ABC
A → Aa | Ad | b
B → Bd | e
C → Cc | g
```

**FIRST Sets:**
- FIRST(A) = {b} (first terminal in non-recursive production)
- FIRST(B) = {e}
- FIRST(C) = {g}
- FIRST(S) = FIRST(A) = {b}

**FOLLOW Sets:**
- FOLLOW(S) = {$}
- FOLLOW(A) = FIRST(B) = {e} (from S → ABC)
- FOLLOW(B) = FIRST(C) = {g} (from S → ABC)
- FOLLOW(C) = FOLLOW(S) = {$} (C is last in S → ABC)

Additionally:
- FOLLOW(A) includes {a, d} (from A → Aa, A → Ad)
- FOLLOW(B) includes {d} (from B → Bd)
- FOLLOW(C) includes {c} (from C → Cc)

**Final:**
- FIRST(S) = {b}, FIRST(A) = {b}, FIRST(B) = {e}, FIRST(C) = {g}
- FOLLOW(S) = {$}, FOLLOW(A) = {e, a, d}, FOLLOW(B) = {g, d}, FOLLOW(C) = {$, c}


# Q218 — FIRST and FOLLOW for A → ε | xAy | AA; S → Sa | aSSb | ε

## Key Points
- Grammar with ε-productions
- A has both recursive and ε productions
- S has left recursion and ε

## Explanation

**Grammar:**
```
A → ε | xAy | AA
S → Sa | aSSb | ε
```

**FIRST Sets:**
- FIRST(A) = {x, ε} (from A → xAy and A → ε; A → AA uses FIRST(A))
- FIRST(S) = {a, ε} (from S → aSSb and S → ε; S → Sa uses FIRST(S))

**FOLLOW Sets:**
- FOLLOW(A) = {y, $} ∪ FIRST(A) - {ε} (from A → xAy and A → AA)
  = {y, $, x}
- FOLLOW(S) = {a, b, $} (from S → Sa, S → aSSb, and end of input)

More precisely:
- FOLLOW(A): From A → xAy, we get {y}. From A → AA, FOLLOW(A) includes FIRST(A) - {ε} ∪ FOLLOW(A) = {x} ∪ FOLLOW(A).
- FOLLOW(S): From S → Sa, we get {a}. From S → aSSb, first S gets FIRST(S) - {ε} ∪ FIRST(S) - {ε} ∪ {b} = {a, b}. 

**Final:**
- FIRST(A) = {x, ε}, FIRST(S) = {a, ε}
- FOLLOW(A) = {x, y, $}, FOLLOW(S) = {a, b, $}


# Q219 — Convert regular expression x*yxx(y+x)* to DFA

## Key Points
- Regular expression: x*yxx(y+x)*
- Build NFA first, then convert to DFA
- Use subset construction

## Explanation

**Regular Expression:** x*yxx(y+x)*

**Step 1: Build NFA**

```
State transitions for x*:
0 --ε--> 1 --x--> 1 (loop)
0 --ε--> 2

Then y:
2 --y--> 3

Then xx:
3 --x--> 4 --x--> 5

Then (y+x)*:
5 --ε--> 6 --y--> 6 (loop)
5 --ε--> 6 --x--> 6 (loop)
5 --ε--> 7 (final)
6 --ε--> 7
```

**Step 2: Convert NFA to DFA (subset construction)**

- q0 = ε-closure(0) = {0, 1, 2}
- q0 on 'x' → {1} → ε-closure = {1, 2}
- q0 on 'y' → {3}
- Continue building states...

**DFA States:**
- q0: {0, 1, 2}
- q1: {1, 2} (on x from q0)
- q2: {3} (on y from q0 or q1)
- q3: {4} (on x from q2)
- q4: {5, 6, 7} (on x from q3)
- q4: {6, 7} (on x or y from q4, loops)

**DFA Transition Table:**

| State | x | y |
|-------|---|---|
| q0 | q1 | q2 |
| q1 | q1 | q2 |
| q2 | q3 | - |
| q3 | q4* | - |
| q4* | q4* | q4* |

Final state: q4


# Q222 — Drawback of CLR and LALR conversion

## Key Points
- CLR has many states (large tables)
- LALR merges states with same core items
- Reduces table size with minimal power loss

## Explanation

**Drawback of CLR Parsing:**

CLR(1) parsers generate a large number of states because each state is distinguished by both item cores and lookahead symbols. This leads to:
- Large parsing tables (memory intensive)
- Longer construction time
- Redundancy when cores are identical but lookaheads differ

**LALR Parsing Solution:**

LALR (Look-Ahead LR) merges CLR states that have identical item cores but different lookaheads. This:
- Significantly reduces the number of states
- Produces more compact parsing tables
- Retains most parsing power (same as CLR for most grammars)

**Conversion Steps (CLR to LALR):**

1. Construct the complete CLR(1) automaton
2. Identify state pairs with identical cores (same items, ignoring lookaheads)
3. Merge these states by taking the union of their lookahead sets
4. Update transitions to merged states
5. Resolve any reduce-reduce conflicts (rare)

The result is an LALR parser with the same states as LR(0) but with lookahead information, providing a practical balance between power and efficiency.


# Q223 — NFA for (0+1)*(00+11)(0+1)* and convert to DFA

## Key Points
- Build NFA for concatenation and Kleene star operations
- Use subset construction for NFA to DFA conversion
- Language: strings containing '00' or '11' substring

## Explanation

**Regular Expression:** (0+1)*(00+11)(0+1)*

This accepts strings containing at least one occurrence of '00' or '11'.

**NFA Construction:**

```
State 0 (start): ε-transitions to state 1
State 1: 0 → 1, 1 → 1 (loop on (0+1)*)
State 1: 0 → 2, 1 → 4 (begin 00 or 11)
State 2: 0 → 6 (complete 00)
State 4: 1 → 6 (complete 11)
State 6: 0 → 6, 1 → 6 (remaining (0+1)*)
State 6: final state
```

**NFA to DFA Conversion:**

| DFA State | NFA States | 0 | 1 |
|-----------|------------|---|---|
| A (start) | {0, 1} | B | C |
| B | {1, 2} | D* | C |
| C | {1, 4} | B | D* |
| D* (final) | {1, 6} | D* | D* |

**Minimal DFA:**
- A: initial state
- B, C: intermediate (seen one symbol)
- D: final (seen 00 or 11)

Once in D, the machine stays in D (all subsequent strings accepted).


# Q257a — Optimize C code: loop with count and result

## Key Points
- Original code has redundant count variable
- increment can be computed directly
- Loop can be unrolled or simplified

## Explanation

**Original Code:**
```c
count=0;
result= 0;
while(count++ < 20) {
    increment= 2*count;
    result +=increment;
}
```

**Optimizations:**

1. **Eliminate redundant variable:**
```c
result = 0;
for(int count = 1; count <= 20; count++) {
    result += 2 * count;
}
```

2. **Strength reduction** (2*count to addition):
```c
result = 0;
int increment = 2;
for(int count = 1; count <= 20; count++) {
    result += increment;
    increment += 2;
}
```

3. **Mathematical simplification:**
Sum = 2(1+2+...+20) = 2 * (20*21/2) = 420

```c
result = 420;
```

**Optimized Code (best):**
```c
result = 420;  // Compile-time computation
```

Or if loop bounds are variable:
```c
result = 0;
int increment = 2;
for(int i = 0; i < 20; i++) {
    result += increment;
    increment += 2;
}
```


# Q257bb — Dead code elimination

## Key Points
- Dead code: code that never executes or whose results are never used
- Compiler optimization removes it
- Improves performance and code size

## Explanation

**Dead Code Elimination (DCE)** is a compiler optimization that removes code fragments that:
1. Are unreachable (never executed)
2. Compute values that are never used
3. Have no effect on program output

**Types of Dead Code:**

1. **Unreachable code:**
```c
if (false) {
    x = 5;  // Dead: condition always false
}

return y;
x = 10;  // Dead: after return
```

2. **Unused computations:**
```c
int a = 5 * 3;  // Dead if 'a' is never read
int b = 10;
// ... b is never used
```

3. **Write-after-write:**
```c
x = 5;
x = 10;  // First assignment is dead
```

**Benefits:**
- Reduced code size
- Faster execution
- Less memory usage
- Cleaner compiled output

DCE is typically performed after other optimizations like constant propagation and control flow analysis, which can expose additional dead code.


# Q258a — Purpose of code optimization and peephole optimization

## Key Points
- Code optimization improves efficiency without changing semantics
- Peephole optimization examines small code windows
- Targets redundant instructions, strength reduction

## Explanation

**Purpose of Code Optimization:**

Code optimization transforms programs to:
- Reduce execution time
- Minimize memory usage
- Decrease code size
- Improve energy efficiency

All while preserving program semantics (behavior).

**Peephole Optimization:**

A technique that examines a small "window" (peephole) of instructions and replaces them with more efficient equivalent sequences.

**Common Peephole Optimizations:**

1. **Redundant load/store elimination:**
```assembly
MOV R1, a
MOV a, R1      // Dead store (eliminated)
```

2. **Constant folding:**
```assembly
MOV R1, 3
ADD R1, 5      // Replace with MOV R1, 8
```

3. **Strength reduction:**
```assembly
MUL R1, 2      // Replace with ADD R1, R1
```

4. **Null sequences:**
```assembly
ADD R1, 0      // Eliminated (no effect)
MUL R1, 1      // Eliminated
```

5. **Algebraic simplification:**
```assembly
SUB R1, R1     // Replace with MOV R1, 0
```

Peephole optimization is applied locally and repeatedly, often revealing new optimization opportunities after each pass.


# Q258b — Loop optimization

## Key Points
- Optimizations targeting iterative constructs
- Common techniques: invariant motion, unrolling, strength reduction
- Significantly impacts performance (loops dominate execution)

## Explanation

**Loop Optimization** applies transformations to reduce overhead and improve efficiency of loops, which typically consume most program execution time.

**Common Loop Optimizations:**

1. **Loop-Invariant Code Motion:**
Move computations that don't change between iterations outside the loop.
```c
// Before
for(i=0; i<n; i++)
    a[i] = x * y;

// After
temp = x * y;
for(i=0; i<n; i++)
    a[i] = temp;
```

2. **Loop Unrolling:**
Replicate loop body to reduce branching overhead.
```c
// Before
for(i=0; i<100; i++)
    a[i] = b[i] + 1;

// After (unroll by 4)
for(i=0; i<100; i+=4) {
    a[i] = b[i] + 1;
    a[i+1] = b[i+1] + 1;
    a[i+2] = b[i+2] + 1;
    a[i+3] = b[i+3] + 1;
}
```

3. **Strength Reduction:**
Replace expensive operations with cheaper ones.
```c
// Before
for(i=0; i<n; i++)
    a[i] = i * 4;

// After
for(i=0, j=0; i<n; i++, j+=4)
    a[i] = j;
```

4. **Loop Fusion/Fission:**
Combine or split loops for better cache locality.


# Q259 — Machine-dependent and machine-independent optimization techniques

## Key Points
- Machine-independent: portable across architectures
- Machine-dependent: exploit specific hardware features
- Both essential for effective optimization

## Explanation

**Machine-Independent Optimizations:**

These work on intermediate representation and are portable across different target machines:

1. **Constant folding:** Evaluate constant expressions at compile-time
2. **Constant propagation:** Replace variables with known constant values
3. **Dead code elimination:** Remove unreachable or unused code
4. **Common subexpression elimination:** Compute shared expressions once
5. **Loop-invariant code motion:** Move unchanging computations out of loops
6. **Inlining:** Replace function calls with function body
7. **Strength reduction:** Replace expensive operations with cheaper ones

**Machine-Dependent Optimizations:**

These exploit specific architectural features:

1. **Register allocation:** Assign variables to limited CPU registers
2. **Instruction selection:** Choose optimal instruction sequences for target
3. **Instruction scheduling:** Reorder instructions to minimize pipeline stalls
4. **Peephole optimization:** Target-specific instruction pattern replacements
5. **Branch prediction hints:** Optimize for specific CPU branch predictors
6. **Cache optimization:** Arrange data/code for cache line alignment
7. **SIMD vectorization:** Use parallel instruction sets (SSE, AVX, NEON)

**Strategy:**

Apply machine-independent optimizations first on IR, then machine-dependent optimizations during code generation for maximum portability and efficiency.


# Q260a — Activation record and runtime storage organization

## Key Points
- Activation record stores function call information
- Contains parameters, local variables, return address, control links
- Related to stack-based storage organization

## Explanation

An **activation record** (or stack frame) is a data structure created for each function invocation, containing all information needed for that execution instance.

**Components of Activation Record:**

1. **Return address:** Where to resume after function returns
2. **Parameters:** Actual arguments passed to function
3. **Local variables:** Function's automatic variables
4. **Saved registers:** Preserved caller's register values
5. **Control link:** Pointer to caller's activation record (dynamic link)
6. **Access link:** Pointer for nested scope access (static link)
7. **Return value:** Space for function result
8. **Temporary storage:** Intermediate computation results

**Runtime Storage Organization:**

```
High Memory
+------------------+
|   Static/Global  |  (allocated at compile time)
+------------------+
|      Heap        |  (dynamic allocation, grows down)
|        ↓         |
|                  |
|        ↑         |
|      Stack       |  (activation records, grows up)
+------------------+
|   Code Segment   |  (program instructions)
+------------------+
Low Memory
```

**Relationship:**

The stack holds activation records in LIFO order. Each function call pushes a new record; each return pops one. This organization supports:
- Recursion (multiple instances of same function)
- Nested function calls
- Automatic memory management for local variables


# Q260b — Applications of DAG (Directed Acyclic Graph)

## Key Points
- DAG represents expressions and computations
- Enables common subexpression detection
- Used in code optimization and generation

## Explanation

**Directed Acyclic Graph (DAG)** in compiler design represents expressions and basic blocks, where:
- Nodes represent operations or operands
- Edges represent data flow
- No cycles exist

**Applications:**

1. **Common Subexpression Elimination:**
DAG automatically shares nodes for identical subexpressions.
```c
a = b + c;
d = b + c;  // Same subexpression

DAG:  +
     / \
    b   c  (shared by both assignments)
```

2. **Dead Code Detection:**
Nodes with no outgoing edges (unreferenced) are candidates for elimination.

3. **Instruction Reordering:**
DAG shows dependencies, allowing independent operations to be reordered for optimization.

4. **Register Allocation:**
Helps determine variable lifetimes and minimize register usage.

5. **Strength Reduction:**
Visualize opportunities to replace expensive operations.

6. **Code Generation:**
Generate optimal instruction sequences by traversing DAG in suitable order.

**Example:**
```c
t1 = a + b;
t2 = a + b;
t3 = t1 + t2;
```

DAG collapses both `a + b` into single node, exposing that t1 and t2 are identical.


# Q261a — Issues in code generation

## Key Points
- Input: intermediate representation (IR)
- Output: target machine code
- Key issues: instruction selection, register allocation, instruction ordering

## Explanation

**Code Generation** translates intermediate representation into executable target code while addressing several challenges:

**Major Issues:**

1. **Instruction Selection:**
Choosing appropriate machine instructions for IR operations. Trade-off between code size and speed.
```
IR: t = a + b
Options: ADD R1, R2, R3  or  LOAD R1, a; ADD R1, b
```

2. **Register Allocation:**
Mapping unlimited IR variables to limited CPU registers. Spilling to memory when needed.
- Minimize memory access
- Handle register conflicts
- Preserve live variables

3. **Instruction Ordering:**
Arranging instructions to minimize pipeline stalls and maximize parallelism.
```
Bad:  LOAD R1, a; USE R1  (stall)
Good: LOAD R1, a; LOAD R2, b; USE R1
```

4. **Memory Management:**
Efficient addressing modes, stack/heap management, alignment requirements.

5. **Control Flow Translation:**
Implementing jumps, branches, loops efficiently.

6. **Target-Specific Features:**
Exploiting special instructions (SIMD), cache optimization, branch prediction.

**Example:**
```c
x = y + z * 2;

// Possible code:
LOAD R1, z
MUL R1, 2
LOAD R2, y
ADD R1, R2
STORE x, R1
```

Goal: Generate correct, efficient code respecting machine constraints.


# Q261b — Basic blocks and flow graphs

## Key Points
- Basic block: maximal sequence of straight-line code
- No branches in (except at start) or out (except at end)
- Flow graph connects basic blocks via control flow

## Explanation

**Basic Block:**

A sequence of consecutive statements where:
- Control enters only at the beginning
- Control leaves only at the end
- No internal branches (straight-line code)

**Identifying Basic Blocks:**

1. Identify **leaders** (first statement of blocks):
   - First instruction in program
   - Target of any jump/branch
   - Instruction immediately after a jump/branch

2. Each leader's block extends to next leader (or end)

**Example:**
```c
1.  x = a + b      // Leader (first statement)
2.  y = x * 2
3.  if (y > 10)    // Branch
4.    z = y - 5    // Leader (branch target)
5.    goto 7
6.  z = y + 5      // Leader (after branch)
7.  w = z * 3      // Leader (jump target)
```

Basic blocks: {1-3}, {4-5}, {6}, {7}

**Flow Graph (Control Flow Graph):**

Directed graph where:
- **Nodes** = basic blocks
- **Edges** = possible control flow between blocks

```
     [B1: 1-3]
       /    \
   [B2: 4-5] [B3: 6]
       \     /
      [B4: 7]
```

**Applications:**
- Data flow analysis
- Loop detection
- Optimization (dead code, reaching definitions)
- Register allocation


# Q262a — Triples, indirect triples, and quadruples

## Key Points
- Three-address code representations
- Trade-offs: space vs. flexibility
- Used in intermediate code generation

## Explanation

These are different formats for representing three-address code:

**1. Quadruples (4-tuple):**
Format: (operator, arg1, arg2, result)

```
Expression: x = (a + b) * (c + d)

(0) (+, a, b, t1)
(1) (+, c, d, t2)
(2) (*, t1, t2, t3)
(3) (=, t3, -, x)
```

**Pros:** Clear, easy to read
**Cons:** Fixed result field wastes space

**2. Triples (3-tuple):**
Format: (operator, arg1, arg2)
Results referenced by position number.

```
(0) (+, a, b)
(1) (+, c, d)
(2) (*, (0), (1))
(3) (=, (2), x)
```

**Pros:** More compact (no explicit temp names)
**Cons:** Hard to reorder (position-dependent)

**3. Indirect Triples:**
Array of pointers to triples, allowing easy reordering.

```
Triples:          Instruction List:
(0) (+, a, b)     [0] -> (0)
(1) (+, c, d)     [1] -> (1)
(2) (*, (0), (1)) [2] -> (2)
(3) (=, (2), x)   [3] -> (3)
```

**Pros:** Combines triple compactness with reordering flexibility
**Cons:** Extra indirection level

**Choice depends on:** optimization needs, memory constraints, and code manipulation requirements.


# Q262b — Three-address code for x := A[y, z]

## Key Points
- Array access: 2D indexing requires address calculation
- Row-major vs. column-major layout
- Multiply indices by element size and dimensions

## Explanation

**Expression:** x := A[y, z] (accessing 2D array element)

**Assumptions:**
- Array A: m rows × n columns
- Base address: base(A)
- Element size: w bytes
- Row-major order (rows stored consecutively)

**Address Calculation:**

Address = base(A) + (y * n + z) * w

**Three-Address Code (Detailed):**

```
t1 = y * n          // Offset for complete rows
t2 = t1 + z         // Add column offset
t3 = t2 * w         // Multiply by element size
t4 = &A             // Base address of array
t5 = t4 + t3        // Compute element address
x = *t5             // Load value at address
```

**Three-Address Code (Compact):**

```
t1 = y * n
t2 = t1 + z
t3 = t2 * w
t4 = A[t3]
x = t4
```

**Alternative (with address operator):**

```
t1 = y * width(A)   // width = n * w
t2 = z * w
t3 = t1 + t2
x = A[t3]
```

The key is computing the linear offset from two indices, considering array dimensions and element size.


# Q263A — Specification of a simple type checker

## Key Points
- Type checker verifies type consistency
- Uses symbol table and type rules

at-ghosh: simply write the last response to file `answers-3.md`

GitHub Copilot: I'll write the answers to the file `answers-3.md`. 

## Q257a — Optimize C code with loop invariant code motion and strength reduction

**Key Points**
- Remove loop-invariant computations (2*count)
- Apply strength reduction (replace multiplication with addition)
- Reduce redundant operations

**Explanation**

The original code recalculates `2*count` in every iteration, which is inefficient. We can optimize by moving invariant code outside the loop and using strength reduction to replace multiplication with addition.

The optimized version eliminates the multiplication operation inside the loop by maintaining `increment` and updating it by 2 each iteration. We also consolidate the loop condition and increment operation.

**Original Code**
```c
count=0;
result= 0;
while(count++ < 20)
{
    increment= 2*count;
    result +=increment;
}
```

**Optimized Code**
```c
count = 0;
result = 0;
increment = 2;
while(count < 20)
{
    count++;
    result += increment;
    increment += 2;
}
```

**Further Optimization**
```c
result = 0;
for(int i = 2; i <= 40; i += 2)
{
    result += i;
}
// Or directly: result = 420; (sum of even numbers 2 to 40)
```

---

## Q257bb — Dead code elimination removes unreachable or unused code

**Key Points**
- Eliminates code that never executes
- Removes computations whose results are never used
- Improves performance and reduces code size

**Explanation**

Dead code elimination is a compiler optimization technique that removes code segments that do not affect program output. This includes unreachable code (after return/break statements), unused variable assignments, and computations whose results are never referenced.

The optimization occurs during the code optimization phase, often after control flow analysis identifies which code blocks are unreachable or which variables are never subsequently used.

**Examples of Dead Code**

```c
// Example 1: Unreachable code
int foo() {
    return 5;
    x = 10;  // Dead code - never executes
}

// Example 2: Unused assignment
void bar() {
    int x = 5;  // Dead code - x never used
    int y = 10;
    printf("%d", y);
}

// Example 3: Unused computation
void baz() {
    int a = 5;
    int b = a * 2;  // Dead code - b never used
    printf("Done");
}
```

**Benefits**
- Reduced executable size
- Faster execution time
- Improved cache performance

---

## Q258a — Code optimization improves efficiency; peephole optimization targets small instruction windows

**Key Points**
- Code optimization reduces execution time and resource usage
- Peephole optimization examines small instruction sequences
- Applies local transformations for improvement

**Explanation**

Code optimization aims to improve program efficiency without changing its semantics. It reduces execution time, memory usage, and power consumption while maintaining correctness.

Peephole optimization is a technique that examines a small "window" (peephole) of instructions and replaces them with faster or shorter equivalent sequences. It operates on small instruction sequences, typically 2-5 instructions, making local improvements.

**Common Peephole Optimizations**

1. **Constant Folding**
```
Before: MOV R1, 3
        ADD R1, 5
After:  MOV R1, 8
```

2. **Strength Reduction**
```
Before: MUL R1, 2
After:  ADD R1, R1
```

3. **Null Sequences**
```
Before: MOV R1, R2
        MOV R2, R1
After:  MOV R1, R2
```

4. **Algebraic Simplification**
```
Before: ADD R1, 0
        MUL R2, 1
After:  (eliminated)
```

5. **Redundant Load/Store**
```
Before: STORE x, R1
        LOAD R1, x
After:  STORE x, R1
```

**Advantages**
- Simple to implement
- Language-independent
- Effective for local improvements

---

## Q258b — Loop optimization improves performance of iterative code structures

**Key Points**
- Targets repetitive code execution
- Includes code motion, unrolling, and reduction
- Significant impact due to repeated execution

**Explanation**

Loop optimization is a critical compiler optimization technique that enhances the performance of loops, which often dominate program execution time. Small improvements inside loops can yield substantial overall performance gains.

**Major Loop Optimization Techniques**

1. **Loop Invariant Code Motion**: Move computations that don't change between iterations outside the loop.

2. **Loop Unrolling**: Reduce loop overhead by executing multiple iterations per loop cycle.

3. **Strength Reduction**: Replace expensive operations with cheaper ones.

4. **Loop Fusion**: Combine adjacent loops with same iteration space.

5. **Loop Fission**: Split loops to improve cache locality.

**Example**
```c
// Before optimization
for(i=0; i<100; i++) {
    x = a * b;  // Loop invariant
    arr[i] = arr[i] + x;
}

// After optimization
x = a * b;  // Moved outside
for(i=0; i<100; i++) {
    arr[i] = arr[i] + x;
}
```

---

## Q259 — Machine-dependent optimizations use hardware features; independent ones are portable

**Key Points**
- Machine-independent: portable across architectures
- Machine-dependent: exploit specific hardware features
- Both improve code efficiency differently

**Explanation**

Code optimization techniques are classified based on their dependency on target machine architecture.

**Machine-Independent Optimizations**

These optimizations work on intermediate code and are portable across different architectures. They focus on algorithmic and structural improvements.

Techniques include:
- Constant folding and propagation
- Dead code elimination
- Common subexpression elimination
- Loop invariant code motion
- Copy propagation
- Algebraic simplifications

**Machine-Dependent Optimizations**

These optimizations exploit specific hardware characteristics like register count, instruction set, pipeline structure, and cache organization.

Techniques include:
- Register allocation and assignment
- Instruction scheduling (pipeline optimization)
- Peephole optimization (machine-specific patterns)
- Instruction selection
- Cache optimization
- Use of specialized instructions (SIMD, vector operations)

**Example**

```c
// Source code
x = a + b + c;

// Machine-independent: CSE
temp = a + b;
x = temp + c;

// Machine-dependent: use specific CPU instructions
// x86: Use ADD with memory operand
// ARM: Use specific register allocation
```

**Trade-offs**
- Independent: Portable but less optimal
- Dependent: Maximum performance but non-portable

---

## Q260a — Activation records store function call data in runtime storage

**Key Points**
- Contains local variables, parameters, return address
- Organized in runtime stack
- Supports nested function calls and recursion

**Explanation**

An activation record (or stack frame) is a data structure that stores information about a single function invocation. When a function is called, an activation record is created and pushed onto the runtime stack; when the function returns, the record is popped.

**Components of Activation Record**
1. Return address
2. Actual parameters
3. Local variables
4. Saved registers
5. Control link (pointer to caller's activation record)
6. Access link (for nested scopes)
7. Temporary storage
8. Return value space

**Runtime Storage Organization**

The runtime memory is divided into:
- **Code segment**: Machine instructions
- **Static/Global area**: Global and static variables
- **Stack**: Activation records (grows downward)
- **Heap**: Dynamic memory allocation (grows upward)

**Example**
```c
void foo(int a, int b) {
    int x = 10;
    int y = bar(a);
    return x + y;
}
```

**Activation Record Layout**
```
+-------------------+
| Return Value      |
+-------------------+
| Return Address    |
+-------------------+
| Parameter b       |
+-------------------+
| Parameter a       |
+-------------------+
| Local var x       |
+-------------------+
| Local var y       |
+-------------------+
| Saved Registers   |
+-------------------+
| Control Link      |
+-------------------+
```

---

## Q260b — DAG applications include code optimization and expression representation

**Key Points**
- Directed Acyclic Graph for expression representation
- Identifies common subexpressions
- Used in code optimization and generation

**Explanation**

DAG (Directed Acyclic Graph) is a powerful intermediate representation used in compilers for various optimization purposes. It represents expressions as a graph where nodes are operators or operands, and edges show data flow.

**Applications of DAG**

1. **Common Subexpression Elimination**: Identifies and eliminates redundant computations by sharing nodes.

2. **Dead Code Elimination**: Detects variables and expressions whose values are never used.

3. **Code Motion**: Identifies loop-invariant expressions that can be moved outside loops.

4. **Constant Folding**: Evaluates constant expressions at compile time.

5. **Algebraic Simplification**: Applies algebraic identities to simplify expressions.

6. **Efficient Code Generation**: Generates optimal instruction sequences.

**Example**
```c
a = b + c;
d = b + c;
e = d + a;
```

**DAG Representation**
```
    e
    |
    +
   / \
  d   a
   \ /
    +
   / \
  b   c
```

The DAG shows that `b + c` is computed once and reused for both `a` and `d`, eliminating the redundant computation.

---

## Q261a — Code generation issues include instruction selection, register allocation, and instruction ordering

**Key Points**
- Instruction selection: choosing optimal machine instructions
- Register allocation: assigning variables to registers
- Instruction ordering: scheduling for pipeline efficiency

**Explanation**

Code generation is the final phase of compilation that produces target machine code. Several critical issues must be addressed to generate efficient executable code.

**Major Issues in Code Generation**

1. **Instruction Selection**: Choosing appropriate machine instructions for intermediate code operations. Different instruction sequences can implement the same operation with varying efficiency.

2. **Register Allocation**: Determining which values to keep in registers versus memory. Registers are fast but limited.

3. **Instruction Ordering**: Arranging instructions to minimize pipeline stalls and maximize parallelism.

4. **Memory Management**: Efficient use of memory hierarchy (registers, cache, RAM).

5. **Addressing Modes**: Selecting appropriate addressing modes for operands.

**Examples**

**Instruction Selection**
```
// Three-address code: t = a + 1
// Option 1: LOAD R1, a
//           ADD R1, #1
// Option 2: LOAD R1, a
//           INC R1
```

**Register Allocation**
```
// Variables: a, b, c, d
// Registers: R1, R2
// Must decide which to keep in registers
t1 = a + b
t2 = c + d
t3 = t1 * t2
```

**Challenges**
- Limited registers
- Balancing code size vs speed
- Target-specific constraints

---

## Q261b — Basic blocks are straight-line code sequences; flow graphs show control flow

**Key Points**
- Basic block: maximal sequence of consecutive instructions
- Entry at beginning, exit at end only
- Flow graph: directed graph of basic blocks

**Explanation**

**Basic Blocks**

A basic block is a sequence of consecutive statements with:
- One entry point (at the beginning)
- One exit point (at the end)
- No internal branches

Control enters at the start and exits at the end without halting or branching except at the end. Basic blocks form the foundation for code optimization.

**Algorithm to Partition into Basic Blocks**

1. Identify leaders (first statement of each block):
   - First statement of program
   - Target of any jump
   - Statement immediately after a jump

2. Each leader's block extends to the next leader or end

**Flow Graph**

A flow graph is a directed graph where:
- Nodes represent basic blocks
- Edges represent possible control flow between blocks

**Example**
```c
1:  i = 1
2:  j = 1
3:  t1 = 10 * i
4:  t2 = t1 + j
5:  t3 = 8 * t2
6:  t4 = t3 - 88
7:  a[t4] = 0.0
8:  j = j + 1
9:  if j <= 10 goto (3)
10: i = i + 1
11: if i <= 10 goto (2)
12: i = 1
13: t5 = i - 1
14: t6 = 88 * t5
```

**Basic Blocks**
- B1: 1-2
- B2: 3-9
- B3: 10-11
- B4: 12-14

---

## Q262a — Triples, indirect triples, and quadruples are intermediate code representations

**Key Points**
- Quadruples: four-field records (op, arg1, arg2, result)
- Triples: three-field records (op, arg1, arg2)
- Indirect triples: use pointer list to reference triples

**Explanation**

These are three-address code representations used by compilers as intermediate forms between source code and target code.

**Quadruples (Quads)**

Four fields: (operator, arg1, arg2, result)

Example: `x = a + b * c`
```
(0) (*,  b,  c,  t1)
(1) (+,  a,  t1, x)
```

**Triples**

Three fields: (operator, arg1, arg2). Result is implicit (position in array).

Example: `x = a + b * c`
```
(0) (*,  b,  c)
(1) (+,  a,  (0))
(2) (=,  (1), x)
```

**Indirect Triples**

Separate instruction list pointing to triples, allowing code reordering without modifying triples.

Example: `x = a + b * c`
```
Triples:              Instruction List:
(0) (*,  b,  c)       (0) -> (0)
(1) (+,  a,  (0))     (1) -> (1)
(2) (=,  (1), x)      (2) -> (2)
```

**Comparison**

| Feature | Quadruples | Triples | Indirect Triples |
|---------|------------|---------|------------------|
| Fields | 4 | 3 | 3 + list |
| Space | More | Less | Medium |
| Optimization | Easy | Hard | Easy |

---

## Q262b — Three-address code for array access x := A[y,z] requires address computation

**Key Points**
- Calculate array element address
- Account for row-major or column-major layout
- Handle multi-dimensional indexing

**Explanation**

For `x := A[y, z]`, we must compute the memory address of element A[y,z]. Assuming row-major order and base address `base`, element size `w`, and dimensions `n1 × n2`.

**Address Calculation**

Address = base + ((y - low1) × n2 + (z - low2)) × w

Where low1, low2 are lower bounds of dimensions.

**Three-Address Code (Assuming 0-based, element size 4)**

**Method 1: Explicit Computation**
```
t1 = y * n2
t2 = t1 + z
t3 = t2 * 4
t4 = base + t3
x = *t4
```

**Method 2: With Base Offset**
```
t1 = y * n2
t2 = t1 + z
t3 = t2 * w
x = A[t3]
```

**Method 3: Optimized**
```
t1 = n2 * w          // Constant, can be precomputed
t2 = y * t1
t3 = z * w
t4 = t2 + t3
t5 = base + t4
x = *t5
```

**For Column-Major Order**
```
t1 = z * n1
t2 = t1 + y
t3 = t2 * w
t4 = base + t3
x = *t4
```

**Note**: The exact code varies based on:
- Array layout (row/column major)
- Index bounds
- Element size
- Target architecture

---

## Q263A — Simple type checker verifies type consistency using rules and symbol table

**Key Points**
- Uses type rules to verify expressions
- Symbol table stores variable types
- Checks assignment compatibility and operator validity

**Explanation**

A type checker ensures that operations are performed on compatible types. A simple type checker works with basic types (int, float, boolean) and applies type inference and checking rules.

**Specification Components**

1. **Type Environment (Symbol Table)**: Maps identifiers to types

2. **Type Rules**: Define valid operations and result types

3. **Type Inference**: Derives expression types from operand types

4. **Type Checking**: Verifies compatibility

**Type Rules**

```
E1 : int, E2 : int
─────────────────── (Add-Int)
E1 + E2 : int

E1 : T, E2 : T
──────────────── (Eq)
E1 == E2 : bool

E : bool
─────────── (Not)
!E : bool

id : T (in symbol table)
──────────────────────── (Var)
id : T
```

**Example Type Checking**

```c
int x = 5;
float y = 3.14;
bool z = x > 10;
x = y;  // Type error
```

**Type Checker Algorithm**
```
1. Build symbol table with declarations
2. For each expression:
   a. Recursively determine operand types
   b. Apply type rules
   c. Check compatibility
   d. Report errors or compute result type
```

---

## Q263B — Procedure calling conventions include call-by-value, call-by-reference, and call-by-name

**Key Points**
- Call-by-value: pass copy of argument
- Call-by-reference: pass address of argument
- Call-by-name: pass argument expression (text substitution)

**Explanation**

Different parameter passing mechanisms affect how arguments are transmitted to procedures and how changes inside procedures affect the caller's environment.

**1. Call-by-Value**

The actual parameter's value is copied to the formal parameter. Changes inside the procedure don't affect the original.

```c
void foo(int x) {
    x = 10;  // Only local x changes
}
int a = 5;
foo(a);      // a still equals 5
```

**Advantages**: Safe, no side effects
**Disadvantages**: Expensive for large structures

**2. Call-by-Reference**

The address of the actual parameter is passed. Changes affect the original variable.

```c
void foo(int *x) {
    *x = 10;  // Original variable changes
}
int a = 5;
foo(&a);      // a now equals 10
```

**Advantages**: Efficient, allows modification
**Disadvantages**: Side effects, aliasing issues

**3. Call-by-Name**

The argument expression is substituted textually into the procedure body (like a macro). Evaluated each time used.

```
procedure foo(x):
    print x
    print x

i = 1
foo(i++)  // Prints 1, then 2 (i++ evaluated twice)
```

**Advantages**: Flexible, lazy evaluation
**Disadvantages**: Complex, inefficient, unexpected behavior

**4. Call-by-Result** (out parameter)
Value passed back after procedure completes.

**5. Call-by-Value-Result** (copy-in/copy-out)
Combination: value copied in, result copied out.

---

## Q264a — Intermediate code for assignments and while statements uses three-address code

**Key Points**
- Assignment: generate code for expression, store result
- While: use labels and conditional jumps
- Three-address code format

**Explanation**

Intermediate code generation translates high-level constructs into simpler three-address code suitable for optimization and target code generation.

**i. Assignment Statements**

For `x = E`, generate code to:
1. Evaluate expression E
2. Store result in x

**Example: x = a + b * c**
```
t1 = b * c
t2 = a + t1
x = t2
```

**Example: x = a[i] + b**
```
t1 = i * element_size
t2 = address(a) + t1
t3 = *t2
t4 = t3 + b
x = t4
```

**ii. While Statements**

For `while (B) S`, generate:
1. Label for loop start
2. Evaluate condition B
3. Conditional jump to exit
4. Execute statement S
5. Unconditional jump to start
6. Label for loop exit

**Syntax-Directed Definition**
```
S → while (B) S1

S.begin = newlabel()
S.after = newlabel()
B.true = newlabel()
B.false = S.after
S1.next = S.begin

S.code = label(S.begin)
          || B.code
          || label(B.true)
          || S1.code
          || gen('goto' S.begin)
          || label(S.after)
```

**Example: while (x < 10) x = x + 1**
```
L1: if x >= 10 goto L2
    t1 = x + 1
    x = t1
    goto L1
L2:
```

---

## Q264b — Syntax-directed definitions for control statements use inherited attributes and labels

**Key Points**
- Use labels for jump targets
- Boolean expressions generate conditional jumps
- Inherited attributes pass label information

**Explanation**

Control statements (if, while, for) are translated using syntax-directed definitions that manage labels and generate appropriate jump instructions.

**Key Attributes**

- **S.next**: Label for code after statement S
- **B.true**: Label to jump to if B is true
- **B.false**: Label to jump to if B is false
- **S.code**: Generated three-address code

**If Statement**

Production: `S → if (B) S1`

```
S.code = B.code
         || label(B.true)
         || S1.code
         || label(S.next)

B.true = newlabel()
B.false = S.next
S1.next = S.next
```

**If-Else Statement**

Production: `S → if (B) S1 else S2`

```
S.code = B.code
         || label(B.true)
         || S1.code
         || gen('goto' S.next)
         || label(B.false)
         || S2.code
         || label(S.next)

B.true = newlabel()
B.false = newlabel()
S1.next = S.next
S2.next = S.next
```

**While Statement**

Production: `S → while (B) S1`

```
S.begin = newlabel()
S.code = label(S.begin)
         || B.code
         || label(B.true)
         || S1.code
         || gen('goto' S.begin)
         || label(B.false)

B.true = newlabel()
B.false = S.next
S1.next = S.begin
```

**Example: if (x < y) z = 1 else z = 2**
```
    if x >= y goto L1
    z = 1
    goto L2
L1: z = 2
L2:
```
